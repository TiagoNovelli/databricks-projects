# ğŸ… Arquitetura MedalhÃ£o - Pipeline

Projeto demonstrando a implementaÃ§Ã£o da **Arquitetura MedalhÃ£o (Medallion Architecture)** utilizando **Delta Lake** e **Apache Spark** no Databricks.

## ğŸ“Š Sobre o Projeto

Este projeto implementa um pipeline de dados em trÃªs camadas (Bronze, Silver, Gold) para processar e analisar dados, demonstrando:

- âœ… Arquitetura MedalhÃ£o (Bronze â†’ Silver â†’ Gold)
- âœ… Formato Delta Lake (Parquet otimizado)
- âœ… TransformaÃ§Ãµes incrementais de dados
- âœ… Qualidade e governanÃ§a de dados
- âœ… AnÃ¡lises e mÃ©tricas de negÃ³cio

## ğŸ—‚ï¸ Estrutura do Projeto

```
data-lakehouse/
â”‚  
â”œâ”€â”€ projeto/
â”‚   â”œâ”€â”€ resource/                 # dados de origem e entrega
â”‚   â”‚   â”œâ”€â”€ inputs/               # arquivos CSV/Parquet recebidos da fonte
â”‚   â”‚   â””â”€â”€ outputs/              # exportaÃ§Ãµes (CSV, Parquet, Excel) para clientes ou setores
â”‚   â”‚
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”‚   â””â”€â”€ 01_bronze_ingestion_dev.py
â”‚   â”‚   â””â”€â”€ prod/
â”‚   â”‚       â””â”€â”€ 01_bronze_ingestion_prod.py
â”‚   â”‚
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”‚   â””â”€â”€ 02_silver_transformation_dev.py
â”‚   â”‚   â””â”€â”€ prod/
â”‚   â”‚       â””â”€â”€ 02_silver_transformation_prod.py
â”‚   â”‚
â”‚   â””â”€â”€â”€â”€ gold/
â”‚       â”œâ”€â”€ dev/
â”‚       â”‚   â””â”€â”€ 03_gold_aggregation_dev.py
â”‚       â””â”€â”€ prod/
â”‚           â””â”€â”€ 03_gold_aggregation_prod.py
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ pipeline_dev.yaml/
â”‚   â””â”€â”€ pipeline_prod.yaml/
â”‚
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ table_schemas.py
â”‚
â””â”€â”€ README.md
```

## ğŸ“ Datasets Utilizados

**Fonte:** Databricks Sample Datasets
- `fly-analysis/` - Atrasos em vÃ´os (Apenas duas tabelas)
- `retail/` - Dados empresariais, varias tabelas (ETL, API, Medallion architecture)
- `credit-card-fraud/` - Fraude em cartÃµes de crÃ©dito (PCA, Ideal para Machine Learning)
- `telemetria/` - Dados de telemetria (Iot)
- `mongodb/` - Dados NoSQL MongoDB

## ğŸ¥‰ Camada Bronze (Raw Data)

**Objetivo:** IngestÃ£o bruta dos dados sem transformaÃ§Ãµes

```python
# Leitura e salvamento em Delta formato
df = spark.read.csv(
    f"dbfs:/databricks-datasets/{PROJETO}",
    header=True,
    inferSchema=True
)

df.write.format("delta") \
    .mode("overwrite") \
    .save(f"/mnt/delta/bronze/{PROJETO}")
```

**CaracterÃ­sticas:**
- Dados originais preservados
- Formato Delta para ACID transactions
- Metadados de ingestÃ£o (timestamp, fonte)
- Um notebook para cada arquivo

## ğŸ¥ˆ Camada Silver (Cleaned Data)

**Objetivo:** Limpeza, validaÃ§Ã£o e enriquecimento
- Regras de negÃ³cio
- DesnormalizaÃ§Ã£o
- Geralmente reduÃ§Ã£o da quantidade de tabelas devido aos joins

```python
from pyspark.sql.functions import col, to_timestamp, when

# Leitura da camada Bronze
df_bronze = spark.read.format("delta").load(f"/mnt/delta/bronze/{PROJETO}")

# TransformaÃ§Ãµes
df_silver = df_bronze \
    .filter(col("delay").isNotNull()) \
    .withColumn("date", to_timestamp(col("date"), "MMddHHmm")) \
    .withColumn("delay_category", 
        when(col("delay") <= 0, "On Time")
        .when(col("delay") <= 15, "Minor Delay")
        .when(col("delay") <= 60, "Moderate Delay")
        .otherwise("Severe Delay")
    ) \
    .dropDuplicates()

# Salvamento em Delta
df_silver.write.format("delta") \
    .mode("overwrite") \
    .option("mergeSchema", "true") \
    .save("/mnt/delta/silver/flight_delays_clean")
```

**CaracterÃ­sticas:**
- RemoÃ§Ã£o de duplicatas, nulos e espaÃ§os
- PadronizaÃ§Ã£o de formatos
- Enriquecimento de dados
- Schema evolution habilitado
- Alguns setores consomem dados dessa camada jÃ¡
- Registro de tabelas no Catalog

## ğŸ¥‡ Camada Gold (Business-Level Aggregations)

**Objetivo:** Dados prontos para consumo analÃ­tico

```python
# MÃ©tricas agregadas por aeroporto de origem
df_gold_origin = spark.read.format("delta") \
    .load("/mnt/delta/silver/flight_delays_clean") \
    .groupBy("origin") \
    .agg(
        count("*").alias("total_flights"),
        avg("delay").alias("avg_delay"),
        max("delay").alias("max_delay"),
        sum(when(col("delay") > 15, 1).otherwise(0)).alias("delayed_flights")
    )

# Join com informaÃ§Ãµes de aeroportos
df_airports = spark.read.format("delta").load("/mnt/delta/silver/airports_clean")

df_gold_final = df_gold_origin.join(
    df_airports,
    df_gold_origin.origin == df_airports.IATA,
    "left"
).select(
    col("origin"),
    col("City"),
    col("State"),
    col("total_flights"),
    col("avg_delay"),
    col("max_delay"),
    col("delayed_flights")
)

df_gold_final.write.format("delta") \
    .mode("overwrite") \
    .save("/mnt/delta/gold/airport_performance")
```

**CaracterÃ­sticas:**
- MÃ©tricas de negÃ³cio agregadas
- Dados otimizados para dashboards
- Particionamento estratÃ©gico

## ğŸš€ Como Executar

### PrÃ©-requisitos
- Databricks Workspace
- Cluster Spark configurado
- Acesso aos databricks-datasets

### ExecuÃ§Ã£o
1. Clone o repositÃ³rio
2. Importe os notebooks para o Databricks
3. Execute na ordem: Bronze â†’ Silver â†’ Gold
4. Execute queries analÃ­ticas

```bash
# Ordem de execuÃ§Ã£o
01_bronze_ingestion.py      # IngestÃ£o inicial
02_silver_transformation.py  # Limpeza e padronizaÃ§Ã£o
03_gold_aggregation.py       # AgregaÃ§Ãµes de negÃ³cio
04_analytics_queries.py      # AnÃ¡lises exploratÃ³rias
```

## ğŸ“ˆ Queries AnalÃ­ticas Exemplo
- CTE sÃ£o muito uteis nas querys SQL para ETL.
```sql
-- Top 10 aeroportos com maiores atrasos mÃ©dios
SELECT 
    origin,
    City,
    State,
    avg_delay,
    total_flights
FROM delta.`/mnt/delta/gold/airport_performance`
ORDER BY avg_delay DESC
LIMIT 10;

-- Taxa de pontualidade por estado
SELECT 
    State,
    SUM(total_flights - delayed_flights) * 100.0 / SUM(total_flights) as on_time_rate
FROM delta.`/mnt/delta/gold/airport_performance`
GROUP BY State
ORDER BY on_time_rate DESC;
```

## ğŸ”§ BenefÃ­cios do Delta Lake

- **ACID Transactions:** Garantia de consistÃªncia
- **Time Travel:** Versionamento de dados
- **Schema Evolution:** AlteraÃ§Ãµes de schema sem quebrar pipeline
- **Upserts eficientes:** MERGE operations
- **CompactaÃ§Ã£o automÃ¡tica:** OtimizaÃ§Ã£o de storage

```python
# Exemplo de Time Travel
df_versao_anterior = spark.read.format("delta") \
    .option("versionAsOf", 1) \
    .load("/mnt/delta/silver/flight_delays_clean")
```

## ğŸ“Š OtimizaÃ§Ãµes Implementadas

- **Z-Ordering:** OrganizaÃ§Ã£o de dados para queries eficientes
- **Particionamento:** Por ano/mÃªs para queries temporais
- **Vacuum:** Limpeza de arquivos antigos
- **Optimize:** CompactaÃ§Ã£o de small files

```python
# OtimizaÃ§Ã£o de tabela Delta
spark.sql("""
    OPTIMIZE delta.`/mnt/delta/gold/airport_performance`
    ZORDER BY (origin, State)
""")
```

## ğŸ“š Conceitos Demonstrados

1. **Arquitetura em Camadas**: SeparaÃ§Ã£o clara de responsabilidades
2. **IdempotÃªncia**: Pipelines podem ser re-executados com seguranÃ§a
3. **Incremental Processing**: Suporte a cargas incrementais
4. **Data Quality**: ValidaÃ§Ãµes e checks de qualidade
5. **GovernanÃ§a**: Auditoria e lineage de dados

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para:
- Reportar bugs
- Sugerir melhorias
- Adicionar novas anÃ¡lises

## ğŸ“ LicenÃ§a

Este projeto Ã© open source e estÃ¡ disponÃ­vel sob a licenÃ§a MIT.

## ğŸ‘¤ Autor

Tiago Novelli - [GitHub Tiago Novelli](https://github.com/TiagoNovelli)

## ğŸ”— Links Ãšteis

- [DocumentaÃ§Ã£o Delta Lake](https://docs.delta.io/)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
- [Databricks Best Practices](https://docs.databricks.com/delta/best-practices.html)
- [Curso Databricks GitHub](https://github.com/TiagoNovelli/curso-databricks)

---

â­ Se este projeto foi Ãºtil, considere dar uma estrela!